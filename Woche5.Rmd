---
title: "Week 5"
author: "Boyko Amarov"
date: "4/19/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(patchwork)

## install.packages("patchwork")
## install.packages("broom")
kids <- read_csv("https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>% select(kid_score, mom_hs)
```

## Lineares Regressionsmodell mit einem kategoriallen Prädiktor 

Variablenbeschreibung:

- `kid_score`: (numeric) Punkte vom IQ Test.
- `mom_hs` (binary): 1 Mutter hat Abitur, 0 Mutter hat keine Abitur

Frage:
Gibt es Unterschiede zwischen den Intelligenztestergebnissen von Kindern mit Müttern mit/ohne Abitur.

Vorschlag: wir spalten den Datensatz nach Gruppenzugehörigkeit (mit/ohne Abitur) und vergleichen die Gruppen 
z.B. anhand des Mittelwertes der Testergebnisse.


```{r}
## == (logischer Vergleich)
kids_mom_nohs <- kids %>% filter(mom_hs == 0)
kids_mom_hs <- kids %>% filter(mom_hs == 1)
```


```{r}
summary(kids_mom_nohs$kid_score)
summary(kids_mom_hs$kid_score)
```

```{r}
# ggplot(data = kids, aes(x = mom_hs, y = kid_score, color = factor(mom_hs))) + 
#   geom_point(position = "jitter")
ggplot(data = kids, aes(x = mom_hs, y = kid_score, colour = factor(mom_hs))) +
  geom_point(position = "jitter") + 
  labs(
    x = "Gruppenzugehörigkeit (ohne Abitur/mit Abitur)",
    y = "Erzielte Punktzahl"
  )
```

```{r}
# ggplot(data = kids, aes(x = mom_hs, y = kid_score, color = factor(mom_hs))) + 
#   geom_point(position = "jitter")
ggplot(data = kids, aes(x = mom_hs, y = kid_score, colour = factor(mom_hs))) +
  geom_point(position = "jitter") + 
  labs(
    x = "Gruppenzugehörigkeit (ohne Abitur/mit Abitur)",
    y = "Erzielte Punktzahl"
  )
```

```{r}
ggplot(data = kids, aes(x = kid_score, y = factor(mom_hs))) + 
  geom_boxplot()
```

$$
i = 1,\ldots,n = 434 \text{ Kinder}\\
y_i: \text{ Testergebnis von Kind } i\\
x_i: \text{ Gruppenzugehörigkeit von Kind (1: Mutter mit Abitur/0: Mutter ohne Abitur) } i\\
$$
$$
y_i = \alpha + \beta x_i + e_i, \quad E(e_i) = 0, i = 1,\ldots,n
$$
$$
E(y_i) = \alpha + \beta x_i\\
\begin{align}
E(y_i) = 
\begin{cases} 
\mu_1 = \alpha + \beta & \text{falls } x_i = 1 \text{ Erwartetes Testergebnis für die Gruppe mit Abitur (der Mutter)} \\
\mu_0 = \alpha & \text{falls } x_i = 0 \text{ Erwartetes Testergebnis für die Gruppe ohne Abitur (der Mutter)}
\end{cases}   
\end{align}\\
\implies\\
\beta = \mu_1 - \mu_0
$$
$\alpha$ ist einfach das erwartete Testergebnis für die Gruppe ohne Abitur (der Mutter). $\beta$ ist einfach die Differenz zwischen den erwarteten Testergebnissen der zwei Gruppen.


```{r}
lm(kid_score ~ 1 + mom_hs, data = kids)
```

$$
\hat{\alpha} = 77.55 \text{ Mitterwert der Testergebnise in der Gruppe ohne Abitur (Mutter)}\\
\hat{\beta} = 11.77 = 89.32 - 77.55 \text{ Differenz der zwei Gruppenmitterwerte} \\
\hat{y}_i =  \hat{\alpha} + \hat{\beta} x_i\\
\hat{y}_i = 77.55 + 11.77 x_i \text{ geschätzte Gleichung}
$$

Das geschätzte Testergebnis von Kindern (Mutter ohne Abitur, $x = 0$) is gleich 77.55 (Punkte). Das geschätzte Testergebnis von Kindern (Mutter mit Abitur, $x = 1$) is um 11.77 Punkte höher als das erwartete Testregebnis der anderen Groupe (Mutter ohne Abitur, $x = 0$).

$$
89.32 - 77.55 = 11.77
$$
## Normalverteilung


### Bedeuting der Parameter der Normalverteilung

Die Dichtefunktion der Normalverteilung mit Parametern $\mu$ und $\sigma$ ist gegeben durch:
$$
f(x) = \frac{1}{\sqrt{\pi \sigma^2}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}, \quad y \in R
$$


wir schreiben $N(\mu, \sigma^2)$. Die Normalverteilung mit Erwartungswert $\mu = 0$ und Standardabweichung $\sigma = 1$ nennen wir die Standardnormalverteilung N(0, 1).

```{r}
## d(density)norm(normal distribution)
## Wert der Dichtefunktion der Normalverteilung mit mu = 0, sigma = 1: N(0, 1)
?dnorm
dnorm(x = 0, mean = 0, sd = 1)

## Wert der Dichtefunktion mit mu = 5, sigma = 2: N(0, 4)
dnorm(x = 1, mean = 5, sd = 2)
```


Graphische Darstellung der Dichtefunktion (Vorbereitung der Daten)

```{r}
norm_df <- expand_grid(
  x = seq(-5, 5, length.out = 300),
  mu = c(0, 1),
  sigma = c(1, 2)
) %>%
  mutate(
    y = dnorm(x, mean = mu, sd = sigma)
  )
```

Graphische Darstellung der Dichtefunktion (Graphik zeichnen)

```{r}
norm_df %>%
  ggplot(aes(x = x, y = y, colour = factor(mu), lty = factor(sigma))) +
  geom_line()

## lty: line type
```
### Berechnung von Wahrscheinlichkeiten (Normalverteilung)


<!-- The area under the density over an interval equals the probability of an event occurring in the interval. -->

<!-- We take the standard normal distribution N(0, 1) and want to compute the probability of x occurring between -1 and 1. -->




```{r}
d <- data.frame(
  x = seq(-5, 5, length.out = 200)
) %>%
  mutate(
    ## Dnorm computes the density 
    y = dnorm(x, mean = 0, sd = 1)
  )

std_norm_plot <- d %>% 
  ggplot(aes(x = x)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(
    data = d %>% filter(x > -1, x < 1),
    aes(ymin = 0, ymax = y), fill = "gray"
  ) +
  geom_line(aes(y = y)) +
  annotate(geom = "text", x = 0, y = 0.2, label = expression("p" %~~% "0.682")) +
  labs(
    x = "x",
    y = "Normal density"
  ) +
  geom_vline(xintercept = -1, lty = 2) + 
  geom_vline(xintercept = 1, lty = 2)

plot(std_norm_plot)
```
```{r}
## p(probability/Wahrscheinlichkeit)norm(normal distribution)
pnorm(-1, mean = 0, sd = 1)
```

$$
P(x < -1|\mu = 0, \sigma = 1) = 0.1586553
$$


```{r}
## p(probability)norm(normal distribution)
pnorm(1, mean = 0, sd = 1)
```

$$
P(x < 1|\mu = 0, \sigma = 1) = 0.8413447
$$
$$
P(-1 < x < 1|\mu = 0, \sigma = 1) = P(x < 1|\mu = 0, \sigma = 1) - P(x < -1|\mu = 0, \sigma = 1) = 0.841 - 0.158 = 0.682
$$

Wir spielen das Spiel beschrieben duch die Standardnormalverteilung 100 Mal.

```{r}
set.seed(123249)

## r(random/zufällig)norm(normal distribution)
norm_sim <- data.frame(
  ## Ziehe 100 Werte aus der Standardnormalverteilung
  x = rnorm(100, mean = 0, sd = 1)
) %>%
  mutate(
    ## &: logisches UND
    x_in_interval = x < 1 & x > -1
  )

## Code for illustration only
std_norm_plot + 
  geom_point(
    data = norm_sim, 
    aes(x = x, y = runif(100, -0.01, 0.01), colour = x_in_interval),
    size = 1,
    alpha = 1/2
  )
# +
#   geom_density(data = norm_sim, color = "steelblue2")
```

```{r}
table(norm_sim$x_in_interval)
```

<!-- ```{r} -->
<!-- d <- data.frame( -->
<!--   x = seq(-5, 5, length.out = 200) -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     ## Dnorm computes the density  -->
<!--     y = dnorm(x, mean = 0, sd = 1) -->
<!--   ) -->

<!-- ## Code for illustration only -->
<!-- d %>%  -->
<!--   ggplot(aes(x = x)) + -->
<!--   # note this next line is the only difference in code from the last plot -->
<!--   geom_ribbon(data = d %>% filter(x > -1, x < 1), -->
<!--               aes(ymin = 0, ymax = y), fill = "gray") + -->
<!--   geom_line(aes(y = y)) + -->
<!--   annotate( -->
<!--     geom = "text",  -->
<!--     x = 0,  -->
<!--     y = 0.2,  -->
<!--     label = substitute( -->
<!--       "Area" %~~% p, -->
<!--       list( -->
<!--         p = round(pnorm(1) - pnorm(-1), 2) -->
<!--       ) -->
<!--     ) -->
<!--   ) + -->
<!--   labs( -->
<!--     x = "x", -->
<!--     y = "Normal density" -->
<!--   ) -->
<!-- ``` -->

```{r}
## Nur zur Illustration
d %>% 
  ggplot(aes(x = x)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(data = d %>% filter(x > 1.96),
              aes(ymin = 0, ymax = y), fill = "gray") +
  geom_line(aes(y = y)) +
  annotate(
    geom = "text", 
    x = 3,
    y = 0.05, 
    label = substitute(
      "p" %~~% p,
      list(
        p = round(1 - pnorm(1.964), 3)
        )
      )
    ) +
  labs(
    x = "x",
    y = "Normal density"
  )
```
$$
P(x > 1.96|\mu = 0, \sigma = 1) = 1 - P(x < 1.96|\mu = 0, \sigma = 1) = 1 - pnorm(1.96, mean = 0, sd = 1)
$$
```{r}
1 - pnorm(1.96, mean = 0, sd = 1)
```


```{r}
d <- data.frame(
  x = seq(-5, 5, length.out = 200)
) %>%
  mutate(
    ## Dnorm computes the density 
    y = dnorm(x, mean = 0, sd = 1)
  )

d %>% 
  ggplot(aes(x = x)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(data = d %>% filter(x > -1, x < 1),
              aes(ymin = 0, ymax = y), fill = "gray") +
  geom_line(aes(y = y)) +
  annotate(geom = "text", x = 0, y = 0.2, label = expression("p" %~~% "??")) +
  labs(x = "x",
       y = "Normal density")
```

## Samples simulation

Let us take the linear regression model and let us _assume_ that the random term $e_i$ follows a normal distribution with zero mean $\mu = 0$ and some standard deviation $\sigma$ (or equivalently variance $\sigma^2$).

$$
y_i = \alpha + \beta x_i + e_i\\
\text{Assume } e_i \sim N(0, \sigma^2)
$$


Assume that we know the function that describes the population of children is:

$$
\begin{align}
  (\#eq:reg-pop)
  y_i = 77.55 + 11.77 x_i + e_i
\end{align}
$$


We would like to study the way the estimates for $\alpha$ and $\beta$ vary from sample to sample. For that purpose we will generate $R = 2000$ samples from the population described by equation \@ref(eq:reg-pop). The we will estimate the regression model for each of these 2000 samples. What follows is a lot of rather technical details that
we need to generate the values for the samples and arrange these values in a way that is convenient for plotting and analysis.

We begin by introducing the `group_by` function and we'll use it to compute the sample group means for the two groups defined by `mom_hs`.

```{r}
## Do the same calculation of the group averages that
## we accomplished by splitting the data in two using `filter`.

kids %>% 
  group_by(mom_hs) %>%
  summarise(
    "Gruppengröße" = n(),
    "Mittelwert" = mean(kid_score)
  )
```

Next we construct a data set using `expand_grid` to replicate the `mom_hs` column $R = 2000$ times.

```{r}
## Number of simulated samples

## Construct a data set with mom_hs repeated R = 2000 times
## Examine the contents of sim_df to see what expand_grid returns

sim_df <- expand_grid(
    R = 1:2000,
    mom_hs = kids$mom_hs
  )
```

After that we calculate the (simulated) IQ score for every child in every sample (a total of $2000 \times 434$ values) by using equation \ref{eq:pop-reg}. For the value of the random term we select a value from the normal distribution with mean 0 and standard deviation $\sigma = 20.41069$ (we will turn to the estimation of $\sigma$ later).

```{r}
sim_df <- sim_df %>%
  mutate(
    ## Compute a simulated IQ score for each kid according to our estimated regression equation
    ## rnorm adds a value selected at random from a normal distribution with mean = 0 and standard
    ## deviation (sigma) = 20.41 (that we estimated from the sample)
    e = rnorm(2000 * 434, mean = 0, sd = 19.85),
    kid_score = 77.55 + 11.77 * mom_hs + e
  )
```

```{r}
stichprobe1 <- sim_df %>% filter(R == 1)
lm(kid_score ~ mom_hs, data = stichprobe1)
head(stichprobe1)
```
```{r}
stichprobe2 <- sim_df %>% filter(R == 2)
tidy(lm(kid_score ~ mom_hs, data = stichprobe2))
head(stichprobe2)
```

The last step completes the simulation of our 2000 samples and we can start estimating the regression model in each sample. For that we group the simulation data set `sim_df` by the simulation number (variable $R$) and run the regression model in every group. The `tidy` function serves to transform the output of `lm` into a format that can easily fit into a rectangular table.

```{r}
# tidy(lm(kid_score ~ mom_hs, data = kids))

sim_coeff <- sim_df %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ mom_hs, data = .)))
```
E.g. `lm` result for the first sample


The data set `sim_coeff` now contains estimated coefficients ($\hat{\alpha}$ and $\hat{\beta}$) for every sample. To plot the distribution of $\hat{\beta}}$ we filter the data set so that we keep only the raw where `term == "mom_hs"` (i.e our estimates for $\beta$).

```{r}
steigungskoeff <- sim_coeff %>% filter(term == "mom_hs") %>% select(R, estimate)

steigungskoeff %>%
  ggplot(aes(x = estimate)) + 
  geom_boxplot() +
  labs(
    x = "Schätzungen für beta",
    title = "Verteilung der Schätzungen für beta (2000 Stichproben)"
  ) + 
  geom_vline(xintercept = 11.77)
```

```{r}
summary(steigungskoeff$estimate)
```

```{r}
sd(steigungskoeff$estimate)
```


Here we use the `summarise_all` function to compute the mean for every variable in the simulation dataset, grouping it by term (remember, we have two terms in the output: the coefficients $\hat{\alpha}$ and $\hat{\beta}$).

```{r}
## Compute the mean of each column by coefficient
sim_coeff %>%
  group_by(term) %>%
  summarise_all(mean)
```

Next we would like to visualise the estimated regression lines for each sample. It is easier if we transform the dataset with the estimated coefficients for each sample are in separate columns. For now we are only interested in the `estimate` variable, so for the sake of simplicity we omit rest of the variables (this is the purpose of the `select` function). The we transform the data set from a long format.

```{r}
sim_coeff_wide <- sim_coeff %>%
  mutate(
    ## Rename the terms to get easier variable names
    term = ifelse(term == "(Intercept)", "alpha", "beta")
  ) %>%
  ## Keep only the R, term and estimate variables
  select(R, term, estimate) %>%
  ## Pivot the data set so that we have the estimated coefficients as columns
  pivot_wider(names_from = "term", values_from = "estimate")
```

Finally, we can plot the regression lines for each sample.

```{r}
sim_coeff_wide %>%
  filter(R < 1500) %>%
  ggplot() + 
    geom_abline(aes(intercept = alpha, slope = beta, group = R), alpha = 0.2, size = 1 / 5) + 
    ## Set the limits of x-axis
    xlim(c(0, 1)) +
    ## Set the limits of y-axis
    ylim(c(65, 95)) +
    labs(
      x = "Educational status of the mother 1: with HS, 0: without HS",
      y = "IQ test score"
    )
```

```{r}
fit <- lm(kid_score ~ mom_hs, data = kids)
fit
summary(fit)
```
- 1. Spalte: geschätzte Koeffizienten: $\hat{\alpha}$, $\hat{\beta}$
- 2. Spalte: Standardfehler der Koeffizientenschätzer. Standardfehler ist die Standardabweichung der Verteilung der Schätzungen der Koeffizienten.

```{r}
sd(steigungskoeff$estimate)
```

```{r}
## Berechnung der Standardabweichung mit Zahlenwerten 
sd(c(0, 2, 20, 200))
sd(c(1, 2, 3, 4))
sd(c(1, 1, 1, 3))
sd(c(1, 1, 1, 1))
# sd(11.771)
```
## Standardfehler von $\hat{\beta}$

Laßt uns die Verteilung von $\hat{\beta}$ unter zwei verschiedenen Simulationen vergleichen. Die zwei Simulationen unterscheiden sich nur durch den Wert von $\sigma$. In der ersten Simulation setzen wir $\sigma = 19.85$ und in der zweiten Simulation setzen wir $\sigma = 40$.

```{r}
## Der Code für die Simulation, nur kompakt geschrieben
## y = 77.5 + 0 x + e
number_of_samples <- 2000
sample_size <- nrow(kids)

sim_1 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 19.85),
    kid_score = 77.54839 + 11.77 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")

sim_2 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 40),
    kid_score = 77.54839 + 11.77 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")
```

```{r}
p1 <- sim_1 %>%
  ggplot(aes(x = estimate, y = 0)) +
    geom_boxplot() +
    geom_point(position = "jitter", size = 1/3, alpha = 0.2) +
    xlim(c(-5, 30)) +
    labs(
      x = "Estimate for beta",
      y = "",
      title = "sigma = 19.85"
    ) +
  scale_y_continuous(breaks = NULL)
p2 <- sim_2 %>%
  ggplot(aes(x = estimate, y = 0)) +
    geom_boxplot() +
    geom_point(position = "jitter", size = 1/3, alpha = 0.2) +
    xlim(c(-5, 30)) +
    labs(
        x = "Beta Hut",
        y = "",
        title = "sigma = 40"
      ) +
    scale_y_continuous(breaks = NULL)

p1 + p2
```

```{r}
sd(sim_1$estimate)
sd(sim_2$estimate)
```


## T-tests

$$
H_0: \beta = 0\\
H_1: \beta \neq 0
$$

$$
y_i = \alpha + \beta x_i + e_i\\
\beta = 0 \implies y_i = \alpha + e_i
$$

Die Aussage $\beta = 0$ bedutet... ?

$$
\beta_0 \iff  \mu_1 = \mu_0
$$

Wir fassen die Stichprobe mit der t-Statistik zusammen.

$$
t = \frac{\hat{\beta} - 0}{\hat{SE}(\hat{\beta})}
$$

$t$ grows with the difference between $\hat{\beta}$ and $0$ and it grows with decreasing
standard error.

$$
t = \frac{11.77}{2.322} = 5.069
$$

## Mögliche Fehlentscheidungen

```{r}
## Code for illustration only
table_data <- expand_grid(
  Reality = c("Schuldig", "Nicht schuldig"),
  Decision = c("Verurteilen", "Nicht verurteilen")
) %>%
  mutate(
    Error = c("-", "Eine schuldige Person freisprechen", "Eine unschuldige Person verurteilen", "-")
  )

table_data %>% 
  knitr::kable() %>%
  kableExtra::kable_styling()
```

```{r}
## Code for illustration only
table_data_h0 <- expand_grid(
  H0 = c("Wahr", "Falsch"),
  Decision = c("Nicht verwerfen", "Verwerfen")
) %>%
  mutate(
    Error = c("-", "Eine wahre Hypothese verwerfen", "Eine falsche Hypothese nicht verwerfen", "-")
  )

table_data_h0 %>% 
  knitr::kable() %>%
  kableExtra::kable_styling()
```

## Stichprobenverteilung von $\hat{\beta}$ und von der t-Statistik under $H_0$

$$
\text{kid_score}_i = 77.55 + \underbrace{\beta}_{ = 0} \text{mom_hs}_i + e_i
$$

```{r}

number_of_samples <- 2000
sample_size <- nrow(kids)

sim_h0 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 19.85),
    ## y = 77.5 + 0 x + e
    kid_score = 77.54839 + 0 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")
```


```{r}
sim_h0 %>%
  ggplot(aes(x = estimate)) +
    geom_boxplot() +
    geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1 / 3) +
    geom_vline(xintercept = 0, colour = "firebrick4") +
    labs(
      title = "Stichprobenverteilung von beta Hut (unter beta = 0)",
      x = "Beta Hut"
    ) +
    # geom_vline(xintercept = c(-1, 1), lty = 2) +
    # geom_vline(xintercept = c(-2, 2), lty = 2, colour = "firebrick4") +
    # geom_vline(xintercept = c(-4, 4), lty = 2, colour = "steelblue") +
    scale_x_continuous(breaks = c(-7, -5, -2, -1, 0, 1, 2, 5, 7))
```

```{r}
sim_h0 %>%
  ggplot(aes(x = statistic)) +
    geom_boxplot() +
    geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1 / 2) +
    geom_vline(xintercept = 0, colour = "firebrick4") +
    labs(
      title = "Stichprobenverteilung der t-statistic (unter beta = 0)",
      x = "t-statistic"
    ) +
    geom_vline(xintercept = c(-1, 1), lty = 2) +
    geom_vline(xintercept = c(-2, 2), lty = 2, colour = "firebrick4") +
    # geom_vline(xintercept = c(-4, 4), lty = 2, colour = "steelblue") +
    scale_x_continuous(breaks = c(-4, -3, -2, -1, 0, 1, 2, 3, 4))
```

```{r}
sum(sim_h0$statistic < -1) + sum(sim_h0$statistic > 1)
```
594 Stichproben (aus 2000 Stichproben) ergaben einen Wert der t-Statistik, der kleiner als -1 oder größer als +1 was.

```{r}
594 / 2000
```

```{r}
sum(sim_h0$statistic < -2) + sum(sim_h0$statistic > 2)
```

```{r}
94 / 2000
```

```{r}
number_of_samples <- 2000
sample_size <- nrow(kids)

sim_h1 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 19.85),
    ## y = 77.5 + 10x + e
    kid_score = 77.54839 + 10 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")
```

```{r}
sim_h1 %>%
  ggplot(aes(x = statistic)) +
    geom_boxplot() +
    geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1 / 3) +
    geom_vline(xintercept = 0, colour = "firebrick4") +
    labs(
      title = "Distribution of the t-statistic (beta = 10)",
      x = "Values of the t-statistic"
    ) +
    # geom_vline(xintercept = c(-1, 1), lty = 2) +
    geom_vline(xintercept = c(-2, 2), lty = 2, colour = "firebrick4", lwd = 1.5) +
    geom_vline(xintercept = c(-4, 4), lty = 2, colour = "steelblue", lwd = 1.5) +
    scale_x_continuous(breaks = c(-4, -2, -1, 0, 1, 2, 4))
```

## t-distribution

If the null hypothesis is true the t-statistic follows a t-distribution with $n - p$ degrees of freedoms, where
p is the number of coefficients in the model. In our example $p = 2$ (one coefficient for the intercept and another coefficient for the slope).

```{r}
sim_h0 %>%
  ggplot(aes(x = statistic)) +
    geom_density(color = "steelblue4") +
    stat_function(fun = dt, n = 101, args = list(df = nrow(kids) - 2)) +
    labs(
      x = "t-statistic"
    )
```
```{r}
## t-distribution hat ein Parameter: Freiheitsgrade (degrees of freedom)

pt(-1.96, df = 434 - 2)
1 - pt(1.96, df = 434 - 2)
```



```{r}
## The degrees of freedom equal the number of observations minus the number of model coefficients
## For the kids dataset df = 434 (observations) - 2 (coefficients: alpha: Intercept, beta: coefficient of mom_hs)

## At 95% significance level
## q: quantile, t: t-distribution: computes a value of x such that the probability P(T < x) = p
qt(0.025, df = 434 - 2)
qt(0.975, df = 434 - 2)

## At 99% significance level
qt(0.01 / 2, df = nrow(kids) - 2)
qt(1 - 0.01 / 2, df = nrow(kids) - 2)
```


```{r}
td_sim <- data.frame(
  x = seq(-4, 4)
)

td_sim %>%
  ggplot(aes(x = x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), colour = "firebrick") +
  stat_function(fun = dt, n = 101, args = list(df = 1), colour = "steelblue") +
  stat_function(fun = dt, n = 101, args = list(df = 10), colour = "steelblue", lty = 2)
```

Critical values derived from the t-distribution
