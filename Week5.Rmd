---
title: "Week 5"
author: "Boyko Amarov"
date: "4/19/2021"
output:
  bookdown::html_document2: default
---

```{r setup, include=FALSE}
## Warning: to compile the notes you need the "bookdown" and the "broom" packages. Install them by
## running install.packages, see the commented lines below

library(tidyverse)
library(broom)
library(patchwork)

## install.packages("patchwork")
## install.packages("bookdown")
## install.packages("broom")

kids <- read_csv("https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>% select(kid_score, mom_hs)
```

# Linear regression model with a single categorical predictor


Variables description:


- `kid_score`: (numeric) Kidâ€™s IQ score.
- `mom_hs` (binary): 1 if the mother has finished high school, 0 otherwise.


Two groups of kids: the ones whose mother had a high school degree (`mom_hs = 1`) and the rest (`mom_hs = 0`).

Question: are this two groups different with respect to IQ (as measured by `kid_score`).


```{r}
# == (logical comparison)
kids_mom_nohs <- kids %>% filter(mom_hs == 0)
kids_mom_hs <- kids %>% filter(mom_hs == 1)
```

```{r}
print("Summary of the no HS degree group")
summary(kids_mom_nohs$kid_score)

print("Summary of the with HS degree group")
summary(kids_mom_hs$kid_score)
```

```{r}
ggplot(data = kids, aes(x = mom_hs, y = kid_score, color = factor(mom_hs))) + 
  geom_point(position = "jitter")
```


```{r}
ggplot(data = kids, aes(x = kid_score, y = factor(mom_hs))) + 
  geom_boxplot()
```
We can also visualise the distribution using histograms:

```{r}
ggplot(data = kids, aes(x = kid_score, color = factor(mom_hs))) + 
  geom_histogram()
```

or density estimates

```{r}
ggplot(data = kids, aes(x = kid_score, color = factor(mom_hs))) + 
  geom_density()
```

Assume that the kids were selected at random from the population of children in the USA (at the time of the survey).


## Summarise the data

```{r}
summary(kids$kid_score)
```
```{r}
table(kids$mom_hs)
```

Assume that the kids were selected at random from the population of all kids in the USA.


There are a lot of samples that we could have observed, but we have selected only _one_ sample!

Assumption about all the possible samples. Before that we formalise our comparison between the two groups
in terms of a linear regression model.

$$
i = 1,\ldots,n = 434 \text{ kids}\\
y_i: \text{ IQ score of kid } i\\
x_i: \text{ mother has high school degree (1) or not (0) for kid } i\\
$$

$$
y_i = \alpha + \beta x_i + e_i, \quad E(e_i) = 0\\
E(y_i) = \alpha + \beta x_i\\
\begin{align}
E(y_i) = 
\begin{cases} 
\mu_1 = \alpha + \beta & \text{if } x_i = 1 \\
\mu_0 =\alpha & \text{if } x_i = 0
\end{cases}   
\end{align}
$$


In the linear model $\alpha$ is the the expected IQ score of kids with mother without high school degree. The expected IQ score of kids with mother with HS degree is $\alpha + \beta$.



$$
\mu_1 = \alpha + \beta\\
\mu_0 =\alpha\\
\implies \beta = \mu_1 - \mu_0
$$

$\beta$ is the difference between expected IQ scores of kids in the first group (mother has HS degree) and expected IQ score of kids in the second group (kids with mother without HS degree).

## Coefficient estimation


```{r}
lm(kid_score ~ 1 + mom_hs, data = kids)
```
Estimated regression equation
$$
\hat{\alpha} = 77.55 \\
\hat{\beta} = 11.77 \\
\hat{y}_i = \hat{\alpha} + \hat{\beta}x_i\\
$$

$$
\hat{y}_i = \underbrace{77.55}_{\text{sample average of the x = 0 group}} + \underbrace{11.77}_{89.32 - 77.55: \text{difference between the group sample averages}}x_i\\
$$

# Normal distribution


## Definition of the normal distribution density

The density function of the normal distribution is given by:
$$
f(x) = \frac{1}{\sqrt{\pi \sigma^2}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}, \quad x \in R
$$

We write $N(\mu, \sigma^2)$ to refer to this distribution. The normal distribution has two parameters: $\mu$ and $\sigma$. It can be shown that $\mu$ is the expected value of the distribution and that $\sigma^2$ is the variance of the distribution.


## Calculate the density (dnorm)

```{r}
## d(density)norm(normal distribution)
## Value of the normal density function with mu = 0, sigma = 1: N(0, 1)
?dnorm
dnorm(x = 0, mean = 0, sd = 1)

## Value of the normal density function with mu = 5, sigma = 2: N(5, 4)
dnorm(x = 0, mean = 5, sd = 2)
```


## Meaning of the parameters

The expected value ($\mu$) is the location parameter of the distribution. It determines the location of the centre of the distribution. The variance of the distribution determines the shape of the curve. Small values of $\sigma$ result in curves that are concentrated around the expected value ($\mu$). Large values fo $\sigma$ result in curves that are less concentrated around the expected value.


To plot of the density function we first create a data set `norm_df` that holds a sequence of x-values between -5 and 5. We use `expand_grid` so that we obtain every combination between `x`, `mu` and `sigma`. The reason is that it is easier to plot the curves with ggplot when we can map variables in the data set to aesthetics like colour and line type.

```{r}
norm_df <- expand_grid(
  # x = c(1, 2),
  x = seq(-5, 5, length.out = 300),
  mu = c(0),
  sigma = c(1/2, 1, 2)
) %>%
  mutate(
    y = dnorm(x, mean = mu, sd = sigma)
  )
```

Here we use the dataset to `norm_df` to plot the curves of the distribution.
```{r}
norm_df %>%
  ggplot(
    aes(
      x = x, 
      y = y, 
      ## Maps the value of mu to the color of the curve (colour)
      colour = factor(mu),
      ## Map the value of sigma to the line type (lty)of the curve
      lty = factor(sigma)
    )
  ) +
  geom_line() + 
  ## Draws two (dashed) vertical lines at x = 0 and at x = 2
  geom_vline(xintercept = c(0, 2), lty = 2) +
  ## Sets the breaks on the x-axis
  scale_x_continuous(breaks = c(-5, 0, 2, 5))
  
## lty: line type
```

## Computing probabilities

<!-- The area under the density over an interval equals the probability of an event occurring in the interval. -->

<!-- We take the standard normal distribution N(0, 1) and want to compute the probability of x occurring between -1 and 1. -->


```{r}
d <- data.frame(
  x = seq(-5, 5, length.out = 200)
) %>%
  mutate(
    ## Dnorm computes the density
    y = dnorm(x, mean = 0, sd = 1)
  )

## Code for illustration only
std_norm_plot <- d %>%
  ggplot(aes(x = x)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(data = d %>% filter(x > -1, x < 1),
              aes(ymin = 0, ymax = y), fill = "gray") +
  geom_line(aes(y = y)) +
  annotate(
    geom = "text",
    x = 0,
    y = 0.2,
    label = substitute(
      "Area" %~~% p,
      list(
        p = round(pnorm(1) - pnorm(-1), 2)
      )
    )
  ) +
  labs(
    x = "x",
    y = "Normal density"
  ) +
  geom_vline(xintercept = -1, lty = 2) + 
  geom_vline(xintercept = 1, lty = 2) +
  scale_x_continuous(breaks = c(-5, -1, 0, 1, 5))

plot(std_norm_plot)
```

```{r}
## p(probability)norm(normal distribution)
pnorm(-1, mean = 0, sd = 1)
```

$$
Area(x < -1|\mu = 0, \sigma = 1) = 0.1586553
$$
```{r}
## p(probability)norm(normal distribution)
pnorm(1, mean = 0, sd = 1)
```

$$
Area(x < 1|\mu = 0, \sigma = 1) = 0.8413447
$$

$$
\begin{align}
(\#eq:prob-center)
Area(-1 < x < 1|\mu = 0, \sigma = 1) = Area(x < 1|\mu = 0, \sigma = 1) - Area(x < -1|\mu = 0, \sigma = 1) = 0.841 - 0.158 = 0.682
\end{align}
$$
Play a standard normal "game" 100 times.

```{r}
## r(random)norm(normal distribution)
norm_sim <- data.frame(
  ## Draw 100 values from the standard normal distribution at random
  x = rnorm(100, mean = 0, sd = 1)
) %>%
  mutate(
    ## &: logical AND
    x_in_interval = x > -1 & x < 1
  )

## Code for illustration only
std_norm_plot + 
  geom_point(
    data = norm_sim, 
    aes(x = x, y = runif(100, -0.01, 0.01), colour = x_in_interval),
    size = 1.5,
    alpha = 1/2
  )
# +
#   geom_density(data = norm_sim, color = "steelblue2")
```

Count how many games end up with a result between -1 and 1 and compare this count to the probability of the interval computed in equation \@ref(eq:prob-center). Run the code a couple of times and observe how the number of games with result in [-1, 1] changes.

```{r}
table(norm_sim$x_in_interval)
sum(norm_sim$x_in_interval) ## or equivalently
```

Commonly we need to compute probabilities for events like (x > 2). To compute it using `pnorm` we will make use of the fact that the entire area under the density curve equals 1.

```{r}
## For illustration only
d %>% 
  ggplot(aes(x = x)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(data = d %>% filter(x > 1.96),
              aes(ymin = 0, ymax = y), fill = "gray") +
  geom_line(aes(y = y)) +
  annotate(
    geom = "text", 
    x = 3,
    y = 0.05, 
    label = substitute(
      "p" %~~% p,
      list(
        p = round(1 - pnorm(2), 3)
        )
      )
    ) +
  labs(
    x = "x",
    y = "Normal density"
  ) + 
  scale_x_continuous(breaks = c(-5, -2, 0, 2, 5))
```

First, compute the area under the curve for the interval $(-\infty, 2)$
```{r}
pnorm(2, mean = 0, sd = 1)
```

The subtract it from 1 to obtain the area under the curve for the interval $(2, \infty)$.

$$
P(x > 2|\mu = 0, \sigma = 1) = 1 - P(x < 2|\mu = 0, \sigma = 1) = 1 - pnorm(2, mean = 0, sd = 1)
$$

```{r}
1 - pnorm(2, mean = 0, sd = 1)
```

## Samples simulation

Let us take the linear regression model and let us _assume_ that the random term $e_i$ follows a normal distribution with zero mean $\mu = 0$ and some standard deviation $\sigma$ (or equivalently variance $\sigma^2$).

$$
y_i = \alpha + \beta x_i + e_i\\
\text{Assume } e_i \sim N(0, \sigma^2)
$$


Assume that we know the function that describes the population of children is:

$$
\begin{align}
  (\#eq:reg-pop)
  y_i = 77.55 + 11.77 x_i + e_i
\end{align}
$$


We would like to study the way the estimates for $\alpha$ and $\beta$ vary from sample to sample. For that purpose we will generate $R = 2000$ samples from the population described by equation \@ref(eq:reg-pop). The we will estimate the regression model for each of these 2000 samples. What follows is a lot of rather technical details that
we need to generate the values for the samples and arrange these values in a way that is convenient for plotting and analysis.

We begin by introducing the `group_by` function and we'll use it to compute the sample group means for the two groups defined by `mom_hs`.

```{r}
## Do the same calculation of the group averages that
## we accomplished by splitting the data in two using `filter`.

kids %>% 
  group_by(mom_hs) %>%
  summarise(
    "Group size" = n(),
    "Average" = mean(kid_score)
  )
```

Next we calculate the _sample_ standard deviation of the IQ test scores (`kid_score`) and for now we will use this value for the standard deviation of the random terms $e_i$.

```{r}
## Estimate the standard deviation of the IQ scores (for all groups)
## so that we can base our simulation on a plausible value
sd(kids$kid_score)
```


Next we construct a data set using `expand_grid` to replicate the `mom_hs` column $R = 2000$ times.

```{r}
## Number of simulated samples

## Construct a data set with mom_hs repeated R = 2000 times
## Examine the contents of sim_df to see what expand_grid returns

sim_df <- expand_grid(
    R = 1:2000,
    mom_hs = kids$mom_hs
  )
```


After that we calculate the (simulated) IQ score for every child in every sample (a total of $2000 \times 434$ values) by using equation \@ref{eq:pop-reg}. For the value of the random term we select a value from the normal distribution with mean 0 and standard deviation $\sigma = 20.41069$ (we will turn to the estimation of $\sigma$ later).

```{r}
## Fixes the random numbers sequence so that you can reproduce the 
## random draws
set.seed(4234)

sim_df <- sim_df %>%
  mutate(
    ## Compute a simulated IQ score for each kid according to our estimated regression equation
    ## rnorm adds a value selected at random from a normal distribution with mean = 0 and standard
    ## deviation (sigma) = 19.85 (that we estimated from the sample)
    # e = rnorm(2000 * 434, mean = 0, sd = 19.85),
    e = rnorm(2000 * 434, mean = 0, sd = 19.85),
    kid_score = 77.54839 + 11.77 * mom_hs + e
  )
```

The last step completes the simulation of our 2000 samples and we can start estimating the regression model in each sample. For that we group the simulation data set `sim_df` by the simulation number (variable $R$) and run the regression model in every group. The `tidy` function serves to transform the output of `lm` into a format that can easily fit into a rectangular table.


```{r}
sample1 <- sim_df %>% filter(R == 1)
lm(kid_score ~ mom_hs, data = sample1)
```

```{r}
sample2 <- sim_df %>% filter(R == 2)
lm(kid_score ~ mom_hs, data = sample2)
tidy(lm(kid_score ~ mom_hs, data = sample2))
```
```{r}
head(sample1)
```

```{r}
head(sample2)
```




```{r}
sim_coeff <- sim_df %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate)
```

The data set `sim_coeff` now contains estimated coefficients ($\hat{\alpha}$ and $\hat{\beta}$) for every sample. To plot the distribution of $\hat{\beta}}$ we filter the data set so that we keep only the raw where `term == "mom_hs"` (i.e our estimates for $\beta$).

```{r}
slopes <- sim_coeff %>%
  filter(term == "mom_hs")

# slopes %>%
#   ggplot(aes(x = estimate)) + 
#   geom_density()
```


```{r}
slopes %>%
  ggplot(aes(x = estimate)) + 
  geom_boxplot() +
  geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1) +
  labs(
    x = "Slope estimate",
    title = "Distribution of slope estimates (over 2000 samples)"
  ) +
  geom_vline(xintercept = 11.77, color = "red") +
  xlim(c(0, 21))
```
```{r}
summary(slopes$estimate)
```
```{r}
sd(slopes$estimate)
```

Here we use the `summarise_all` function to compute the mean for every variable in the simulation dataset, grouping it by term (remember, we have two terms in the output: the coefficients $\hat{\alpha}$ and $\hat{\beta}$).

```{r}
## Compute the mean of each column by coefficient
sim_coeff %>%
  group_by(term) %>%
  summarise_all(mean)
```

Next we would like to visualise the estimated regression lines for each sample. It is easier if we transform the dataset with the estimated coefficients for each sample are in separate columns. For now we are only interested in the `estimate` variable, so for the sake of simplicity we omit rest of the variables (this is the purpose of the `select` function). The we transform the data set from a long format.

```{r}
sim_coeff_wide <- sim_coeff %>%
  mutate(
    ## Rename the terms to get easier variable names
    term = ifelse(term == "(Intercept)", "alpha", "beta")
  ) %>%
  ## Keep only the R, term and estimate variables
  select(R, term, estimate) %>%
  ## Pivot the data set so that we have the estimated coefficients as columns
  pivot_wider(names_from = "term", values_from = "estimate")
```

Finally, we can plot the regression lines for each sample.

```{r}
sim_coeff_wide %>%
  filter(R < 100) %>%
  ggplot() + 
    geom_abline(aes(intercept = alpha, slope = beta, group = R), alpha = 0.1, size = 1 / 5) + 
    ## Set the limits of x-axis
    xlim(c(0, 1)) +
    ## Set the limits of y-axis
    ylim(c(65, 95)) +
    labs(
      x = "Educational status of the mother 1: with HS, 0: without HS",
      y = "IQ test score"
    )
```

```{r}
fit <- lm(kid_score ~ mom_hs, data = kids)
fit
summary(fit)
```
- Estimate: $\hat{\alpha}$, $\hat{\beta}$
- Standard error of the estimate is the standard deviation of the estimator for the coefficient.

```{r}
sd(slopes$estimate)
```


```{r}
## Standard deviation
sd(c(1, 2, 200, 1000))
sd(c(1, 1, 1, 3))
sd(c(1, 1, 1, 1))
# sd(11.771)
```

## Standard error of $\hat{\beta}$

We compare the distributions of $\hat{\beta}$ with two different simulation. The first one using $\sigma = 19.85$ and the second one $\sigma = 40$

```{r}
## Here we simply write the whole simulation in a more compact way
## y = 77.5 + 0 x + e
number_of_samples <- 2000
sample_size <- nrow(kids)

sim_1 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 19.85),
    kid_score = 77.54839 + 11.77 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")

sim_2 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 40),
    kid_score = 77.54839 + 11.77 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")
```

```{r}
p1 <- sim_1 %>%
  ggplot(aes(x = estimate, y = 0)) +
    geom_boxplot() +
    geom_point(position = "jitter", size = 1/3, alpha = 0.2) +
    xlim(c(-5, 30)) +
    labs(
      x = "Estimate for beta",
      y = "",
      title = "sigma = 19.85"
    ) +
  scale_y_continuous(breaks = NULL)
p2 <- sim_2 %>%
  ggplot(aes(x = estimate, y = 0)) +
    geom_boxplot() +
    geom_point(position = "jitter", size = 1/3, alpha = 0.2) +
    xlim(c(-5, 30)) +
    labs(
        x = "Estimate for beta",
        y = "",
        title = "sigma = 40"
      ) +
    scale_y_continuous(breaks = NULL)

p1 
# + p2
```

```{r}
sd(sim_1$estimate)
sd(sim_2$estimate)
```


## Hypothesis tests

### t-tests

$$
H_0: \beta = 0\\
H_1: \beta \neq 0
$$

$$
y_i = \alpha + \beta x_i + e_i\\
\beta = 0 \implies y_i = \alpha + e_i
$$






















Implies that the difference between the expected group average scores (in the population) is zero (no difference).



$$
\beta_0 \iff  \mu_1 = \mu_0
$$

A statistic (function of the data) that summarises the evidence against the null hypothesis.

$$
t = \frac{\hat{\beta} - 0}{\hat{SE}(\hat{\beta})}
$$
$t$ grows with the difference between $\hat{\beta}$ and $0$ and it grows with decreasing
standard error.

$$
t = \frac{11.77}{2.322} = 5.069
$$
## Types of errors

```{r}
## Code for illustration only
table_data <- expand_grid(
  Reality = c("Guilty", "Not guilty"),
  Decision = c("Convict", "Not convict")
) %>%
  mutate(
    Error = c("-", "Fail to sentence a guilty person", "Sentence an innocent person", "-")
  )

table_data %>% 
  knitr::kable() %>%
  kableExtra::kable_styling()
```

```{r}
## Code for illustration only
table_data_h0 <- expand_grid(
  H0 = c("True", "False"),
  Decision = c("Not reject", "Reject")
) %>%
  mutate(
    Error = c("-", "Reject a true null hypothesis", "Fail to reject a false null hypothesis", "-")
  )

table_data_h0 %>% 
  knitr::kable() %>%
  kableExtra::kable_styling()
```





How does this statistic vary from sample to sample? We will answer this with a simulation:

$$
\text{kid_score}_i = 77.55 + \underbrace{\beta}_{ = 0} \text{mom_hs}_i + e_i
$$

```{r}
## y = 77.5 + 0 x + e
number_of_samples <- 2000
sample_size <- nrow(kids)

sim_h0 <- expand_grid(
    R = 1:number_of_samples,
    mom_hs = kids$mom_hs
  ) %>%
  mutate(
    ## Compute a simulated IQ score for each kid according to our estimated regression equation
    ## rnorm adds a value selected at random from a normal distribution with mean = 0 and standard
    ## deviation (sigma) = 19.85 (that we estimated from the sample)
    e = rnorm(number_of_samples * sample_size, mean = 0, sd = 19.85),
    kid_score = 77.54839 + 0 * mom_hs + e
  ) %>%
  group_by(R) %>%
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, statistic) %>%
  filter(term == "mom_hs")
```

```{r}
sim_h0 %>%
  ggplot(aes(x = estimate)) +
    geom_boxplot() +
    geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1 / 3) +
    geom_vline(xintercept = 0, colour = "firebrick4") +
    labs(
      x = "estimates for beta (real beta = 0)"
    )
```

```{r}
sim_h0 %>%
  ggplot(aes(x = statistic)) +
    geom_boxplot() +
    geom_point(aes(y = 0), position = "jitter", alpha = 0.2, size = 1 / 3) +
    geom_vline(xintercept = 0, colour = "firebrick4") +
    labs(
      title = "Distribution of the t-statistic",
      x = "Values of the t-statistic"
    ) +
    geom_vline(xintercept = c(-1, 1), lty = 2) +
    geom_vline(xintercept = c(-2, 2), lty = 2, colour = "firebrick4") +
    scale_x_continuous(breaks = c(-2, -1, 0, 1, 2))
```

```{r}
sum(sim_h0$statistic < -1) + sum(sim_h0$statistic > 1)
```
610 samples (out of 2000 samples) gave a value of the t-statistic less than -1 or greater than +1.

```{r}
610 / 2000
```
```{r}
sum(sim_h0$statistic < -2) + sum(sim_h0$statistic > 2)
```
```{r}
86 / 2000
```


```{r}
summary(sim_h0$statistic)
quantile(sim_h0$statistic, prob = c(0.025, 0.25, 0.5, 0.75, 0.975))
```

```{r}
qt(0.025, df = nrow(kids) - 2)
```


## t-distribution

If the null hypothesis is true the t-statistic follows a t-distribution with $n - p$ degrees of freedoms, where
p is the number of coefficients in the model. In our example $p = 2$ (one coefficient for the intercept and another coefficient for the slope).

```{r}
sim_h0 %>%
  ggplot(aes(x = statistic)) +
    geom_density(color = "steelblue4") +
    stat_function(fun = dt, n = 101, args = list(df = nrow(kids) - 2)) +
    labs(
      x = "t-statistic"
    ) 
```


```{r}
td_sim <- data.frame(
  x = seq(-4, 4)
)

td_sim %>%
  ggplot(aes(x = x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), colour = "firebrick") +
  stat_function(fun = dt, n = 101, args = list(df = 1), colour = "steelblue") +
  stat_function(fun = dt, n = 101, args = list(df = 10), colour = "steelblue", lty = 2)
```



<!-- Plot the distributions of scores for each simulated sample -->
<!-- ```{r} -->
<!-- sim_df %>% -->
<!--   filter(R < 50) %>% -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = kid_score,  -->
<!--       group = interaction(R, mom_hs), -->
<!--       colour = factor(mom_hs) -->
<!--       ) -->
<!--     ) + -->
<!--   geom_density( -->
<!--     size = 1/5 -->
<!--   ) -->
<!-- ``` -->
